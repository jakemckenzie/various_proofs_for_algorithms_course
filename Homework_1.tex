
\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\begin{document}
\title{TCSS 343 - Assignment 1}
\maketitle


\section{2.1 Understand}

Prove the theorem below. 
Use a \textbf{direct proof} to find constants that satisfy the definition of \begin{math}\Theta(n^2)\end{math} or use the \textbf{limit test}.  
Make sure your proof is complete, concise, clear and precise.
\\\\\textbf{Theorem 1.} 
\begin{math}n^5-64n^3-n^2 \epsilon \Theta(n^5)\end{math}
\\\\For the first theorem a limit test was used to ensure that polynomial given was \begin{math}\Theta(n^5)\end{math}. For most limits of 
this nature it is best to factor the polynomial, this usually results in a short elegant solution. 
\[ \lim_{n \to \infty}  \frac{n^5-64n^3-n^2}{n^5} = \lim_{n \to \infty} \frac{n^5}{n^5}\Bigg(\frac{1-\frac{64}{n^2}-\frac{1}{n^3}}{1}\Bigg) = \lim_{n \to \infty} 1\Bigg(\frac{1-0-0}{1}\Bigg) = \lim_{n \to \infty} 1 = 1\]
\\Since the limit converged to 1, a positive constant value, we may say that theorem 1 is indeed true.\\\\


\noindent Prove Gauss’s sum using induction on \begin{math}n\end{math}. Make sure to include a base case for \begin{math} n = 1\end{math} and an inductive hypothesis and an inductive step for \begin{math}n > 1\end{math}.\\

\noindent \textbf{Theorem 2.}
\begin{math}P(n):\sum\limits_{i = 1}^{n} i = \frac{n(n + 1)}{2} \forall n \epsilon \mathbb{Z}_+  \end{math}\\
\\\\For this proof I used the fundamental theorem of algebra to help assist me. I hope that the steps are clear.\\
\begin{enumerate}
\item Base Case:\\
\[P(1):\sum\limits_{i = 1}^{1} i = 1  = \frac{1(1+1)}{2} = \frac{2}{2} = 1\]
\item Inductive Hypothesis:
\[P(k):\sum\limits_{i = 1}^{k} i = \frac{k(k + 1)}{2} \forall k \epsilon \mathbb{Z}_+\]
\item Inductive Step:

\begin{align*}
P(k+1):\sum\limits_{i = 1}^{k+1} i &= \frac{(k + 1)(k+2)}{2} \forall k \epsilon \mathbb{Z}_+\\
\sum\limits_{i = 1}^{k} i + (k + 1)&= \frac{(k + 1)(k+2)}{2}\\
\frac{k(k+1)}{2} + (k+1)&= \frac{(k + 1)(k+2)}{2}\\
\frac{k(k+1)}{2} + \frac{2(k+1)}{2}&= \frac{(k + 1)(k+2)}{2}\\
\frac{k(k+1)+ 2(k+1)}{2}&= \frac{(k + 1)(k+2)}{2}\\
\frac{(k + 1)(k+2)}{2}&= \frac{(k + 1)(k+2)}{2}\\
&\hspace{0.3cm}\qedsymbol
\end{align*}
\end{enumerate}
\noindent Prove Gauss’s sum using induction on \begin{math}n\end{math}. Make sure to include a base case for \begin{math} n = 1\end{math} and an inductive hypothesis and an inductive step for \begin{math}n > 1\end{math}.\\\\
\noindent \textbf{Theorem 3.}
\begin{math}P(n):\sum\limits_{i = 1}^{n} i^5 = \big(\frac{n(n + 1)}{2}\big)^2\frac{2n^2+2n-1}{3} \forall n \epsilon \mathbb{Z}_+  \end{math}
\\\\For this proof I used the fundamental theorem of algebra to help assist me. The last three steps of the inductive step to ensure \begin{math}P(k+1)\end{math} step I obtained those steps by expanding the right hand side and ensuring that each polynomial on the left hand side matched the right hand side. Those were included on the left hand side in reverse to ensure continuity.\\
\begin{enumerate}

\item Base Case:\\
\begin{align*}
P(1):\sum\limits_{i = 1}^{1} i^5 &= \big(\frac{1(1 + 1)}{2}\big)^2\frac{2(1)^2+2(1)-1}{3}\\
1^5 &= \big(\frac{1(2)}{2}\big)^2\frac{3}{3}\\
1 &= 1
\end{align*}

\item Inductive Hypothesis:
\begin{align*}
&P(k):\sum\limits_{i = 1}^{k} i^5 = \big(\frac{k(k + 1)}{2}\big)^2\frac{2k^2+2k-1}{3} \forall k \epsilon \mathbb{Z}_+
\end{align*}
\item Inductive Step:
\begin{align*}
P(k+1):\sum\limits_{i = 1}^{k+1} i^5 = \big(\frac{(k+1)(k + 1 + 1)}{2}\big)^2\frac{2(k+1)^2+2(k+1)-1}{3} \forall k \epsilon \mathbb{Z}_+ \\
\sum\limits_{i = 1}^{k} i^5 + (k + 1)^5 = \big(\frac{(k+1)(k + 2)}{2}\big)^2\frac{2k^2+5k+2}{3} \\
\bigg(\frac{k(k + 1)}{2}\bigg)^2\frac{2k^2+2k-1}{3} + (k + 1)^5 &=\\
\bigg(\frac{1}{12}\bigg)[k^2(k+1)^2(2k^2+2k-1)+12(k+1)^5]&= \\
\bigg(\frac{1}{12}\bigg)[(k^4+2k^3+k^2)(2k^2+2k-1)+12(k+1)^5]&=\\
\end{align*}
\begin{align*}
\bigg(\frac{1}{12}\bigg)(2k^6+6k^5+5k^4-k^2+12k^5+60k^4+120k^3+120k^2+60k+12))&=\\
\bigg(\frac{1}{12}\bigg)(2k^6+18k^5+65k^4+120k^3+119k^2+60k+12))&=\\
\bigg(\frac{1}{12}\bigg)(k^4+6k^3+13k^2+12k+4)(2k^2+6k+3)&=\\
\bigg(\frac{1}{12}\bigg)(k^2+3k+2)^2(2k^2+6k+3)&=\\
\big(\frac{(k+1)(k + 2)}{2}\big)^2\frac{2k^2+5k+2}{3}&=\\
&\hspace{0.3cm}\qedsymbol
\end{align*}
\end{enumerate}
Prove by induction that for all natural numbers \begin{math}x\end{math} and \begin{math}n\end{math}, \begin{math}x^n - 1\end{math} is divisible by \begin{math}x - 1\end{math}.\\\\
\noindent For this proof I decided to show that \begin{math}(x^n - 1)\bmod{x - 1}\equiv 0\end{math} because that statement is equivalent to saying \begin{math}x^n - 1\mid x - 1\end{math}.\\\\
\noindent Show \begin{math}\forall x,n \epsilon \mathbb{Z}_+ ;P(x,n):(x^n - 1)\bmod{x - 1}\equiv 0\end{math} 
\begin{enumerate}
\item Base Case:\\
\[P(1,1):(1^1 - 1)\bmod{1 - 1}\equiv 0 \mod 0 \equiv 0\]\\
This is assuming that we define \begin{math}a \mod 0 \equiv 0\end{math} where a may be any integer. This answer depends on how we decide to define the modulus function. This is assuming we define the modulus to be the distance of any two integers from each other. In many modern programming languages and applications this operation is undefined. I see no other way to complete this proof without ignoring this limitation and using the more liberal definition.
\item Inductive Hypothesis:
\[P(a,b):(a^b - 1)\bmod{a - 1}\equiv 0\ ; \forall a,b \epsilon \mathbb{Z}_+\]
\item Inductive Step:\\
For this proof I made heavy use of the binomial theorem, specifically the power expansion on \begin{math}(1+x)^n\end{math}.
\begin{align*}
P(a+1,b+1):((a+1)^{b+1} - 1)\bmod{(a + 1 - 1)}&\equiv 0\ ; \forall a,b \epsilon \mathbb{Z}_+\\
((a+1)(a+1)^b - 1)\bmod{a}&\equiv 0\\
((a+1)\sum\limits_{i = 0}^{b} {b \choose i} a^i - 1)\bmod{a}&\equiv 0\\
(\sum\limits_{i = 0}^{b} {b \choose i} a^{i+1} + \sum\limits_{i = 0}^{b} {b \choose i} a^i - 1)\bmod{a}&\equiv 0\\
(\sum\limits_{i = 0}^{b} {b \choose i} a^{i+1} + \sum\limits_{i = 1}^{b - 1} {b \choose i} a^i + {b \choose b}(a^b + 1) - 1)\bmod{a}&\equiv 0\\
(\sum\limits_{i = 0}^{b} {b \choose i} a^{i+1} + \sum\limits_{i = 1}^{b - 1} {b \choose i} a^i + (1)(a^b + 1 - 1)\bmod{a}&\equiv 0\\
(\sum\limits_{i = 0}^{b} {b \choose i} a^{i+1} + \sum\limits_{i = 1}^{b - 1} {b \choose i} a^i + a^b)\bmod{a}&\equiv 0\\
((\sum\limits_{i = 0}^{b} {b \choose i} a^{i+1})\bmod{a} + (\sum\limits_{i = 1}^{b - 1} {b \choose i} a^i)\bmod{a} + (a^b)\bmod{a})\bmod{a}&\equiv 0\\
(0 + 0 + 0)\bmod{a}&\equiv 0\\
0\bmod{a}&\equiv 0\\
0&\equiv 0\\
&\hspace{0.3cm}\qedsymbol
\end{align*}
\end{enumerate}
Prove by induction that \begin{math}x^n - y^n\end{math} is divisible by \begin{math}x - y\end{math} for all natural numbers x, y (with \begin{math}x \not = y\end{math}), and n.\\\\
\noindent Show \begin{math}\forall x,y,n \epsilon \mathbb{Z}_+ ; P(x,y,n):(x^n-y^n)\bmod{x-y} \equiv 0\end{math}
\begin{enumerate}
\item Base Case:\\
\[P(1,1,1):(1^1 - 1^1)\bmod{1 - 1}\equiv 0 \mod 0 \equiv 0\]\\
This is assuming that we define \begin{math}a \mod 0 \equiv 0\end{math} where a may be any integer. This answer depends on how we decide to define the modulus function. This is assuming we define the modulus to be the distance of any two integers from each other. In many modern programming languages and applications this operation is undefined. I see no other way to complete this proof without ignoring this limitation and using the more liberal definition.
\item Inductive Hypothesis:
\[P(a,b,c):(a^c - b^c)\bmod{a - b}\equiv 0\ ; \forall a,b,c \epsilon \mathbb{Z}_+\]
\item Inductive Step:\\
\begin{align*}
P(a+1,b+1,c+1):((a+1)^{c+1} - (b+1)^{c+1})\bmod{((a + 1) - (b + 1))}&\equiv 0 ;\\
\forall a,b,c \epsilon \mathbb{Z}_+\\
((a+1)^{c+1} - (b+1)^{c+1})\bmod{((a + 1) - (b + 1))}&\equiv 0\\
((a+1)(a+1)^{c} - (b+1)(b+1)^{c})\bmod{(a-b)}&\equiv 0\\
((a+1)\sum\limits_{i = 0}^{c} {c \choose i} (a^i - 1) - (b+1)\sum\limits_{j = 0}^{c} {c \choose j} (b^j - 1))\bmod{(a-b)} &\equiv 0\\
(\sum\limits_{i = 0}^{c} {c \choose i} a^{i+1} - \sum\limits_{j = 0}^{c} {c \choose j} b^{j+1} + \sum\limits_{i = 0}^{c} {c \choose i} a^i - \sum\limits_{j = 0}^{c} {c \choose j} b^j)\bmod{(a-b)} &\equiv 0\\
(\sum\limits_{i = 0}^{c} {c \choose i} a^{i+1} - \sum\limits_{j = 0}^{c} {c \choose j} b^{j+1} + \sum\limits_{i = 1}^{c} {c \choose i} a^i - \sum\limits_{j = 1}^{c} {c \choose j} b^j + c - c)\bmod{(a-b)} &\equiv 0\\
((\sum\limits_{i = 0}^{c} {c \choose i} a^{i+1} - \sum\limits_{j = 0}^{c} {c \choose j} b^{j+1})\bmod{(a-b)} +\\ (\sum\limits_{i = 1}^{c} {c \choose i} a^i - \sum\limits_{j = 1}^{c} {c \choose j} b^j)\bmod{(a-b)})\bmod{(a-b)} &\equiv 0\\
(((\sum\limits_{i = 1}^{c} {c \choose i} a^{i + 1} - \sum\limits_{j = 1}^{c} {c \choose j} b^{j + 1}) + (1 - 1)\bmod{(a-b)} +\\ ((\sum\limits_{i = 1}^{c} {c \choose i} a^{i} - \sum\limits_{j = 1}^{c} {c \choose j} b^{j}))\bmod{(a-b)})\bmod{(a-b)} &\equiv 0\\
\end{align*}
\begin{align*}
(0 + 0) &\equiv 0\\
0 &\equiv 0 \\
&\hspace{0.3cm}\qedsymbol
\end{align*}
\end{enumerate}
Some explanation is needed to explain how the summations on the last page are divisible by \begin{math} a - b\end{math}. Due to the binomial theorem, any series of the nature \begin{math} (a - b) + (a^2 - b^2) + (a^3 - b^3) + ...  \end{math} has within it an \begin{math} (a - b) \end{math}. So if the first term 1 is allowed to elapse from each term we can then say the summations are divisible.\\\\
Prove the following theorem using the definition of \begin{math} \Omega (f (n))\end{math} and \begin{math}O(f (n))\end{math} , or else the limit test.\\\\
\noindent \textbf{Theorem 4.} Let \begin{math}d \geq 2\end{math} be an integer. Then \begin{math}\sqrt[^d]{n} \epsilon \Omega (\sqrt[^{2d}]{n})\end{math} and \begin{math}(\ln {n}^d) \epsilon O(\sqrt{n})\end{math}\\\\
\begin{align}
\lim_{n\to\infty} \frac{n^{\frac{1}{d}}}{n^{\frac{1}{2d}}} &= \lim_{n\to\infty} n^{\frac{1}{d}} n^{\frac{-1}{2d}}\\
&= \lim_{n\to\infty} n^{\frac{1}{d}-\frac{1}{2d}}\\
&= \lim_{n\to\infty} n^{\frac{1}{2d}} \rightarrow \infty
\end{align}\\
Use the fact that for any \begin{math} n  \geq 1\end{math} by the Cauchy-Sqwarz inequality\
\[\ln{n} = \int_{1}^{n} \frac{dx}{x} \leq \sqrt{\int_{1}^{n} dx \int_{1}^{n} \frac{dx}{x^2}} = \sqrt{(n-1)(1-\frac{1}{n})} = \sqrt{n + \frac{1}{n} - 2}\]\\
Producing a desired result of:
\[\ln{n} \leq \sqrt{n + \frac{1}{n} - 2}\]
Using the result of prior limit test and immediate results above (since \begin{math}d \geq 2\end{math}), we may say that:
\[\ln{n} < \sqrt[2d]{n} < \sqrt{n + \frac{1}{n} - 2} < \sqrt{n}\]
Thus:
\[\lim_{n\to\infty} \frac{(\ln{n})^d}{\sqrt{n}} \rightarrow 0\]
\section{2.1 Understand}
Place these functions in order from slowest asymptotic growth to fastest asymptotic growth. You will want to simplify them algebraically before comparing them. Give a short justification (a proof is not necessary) of how you came to this ordering.\\\\\
According to Donald Knuth, in Concrete Mathematics page 426 and 427, hierarchy and asymptotics of functions was introduced by Paul du Bois-Reymond in 1871 with the following definition:\\\\ 
\[f(n) < g(n) \Longleftrightarrow \lim_{n\to\infty} \frac{f(n)}{g(n)} = 0\]
Knuth continues on to give a what he calls,"asymptotic pecking order"(where \begin{math}0 < \epsilon < 1 < c\end{math}) which I used to solve this problem:
\[1 < \log{\log{n}} < \log{n} < n^{\epsilon} < n^c < n^{log{n}} < c^n < n^n < c^{c^{n}}\]
Using this pecking order, I made educated guesses as to which functions grew slower than others, then used Mathematica to check these guesses.
\begin{enumerate}
\item \begin{math}f_7(n) = \log{\log{n-1}}\end{math}\\
The above function is clearly smaller than the function below it. Applying a logarithm to another logarithm significantly slows down the asymptotic growth.
\item \begin{math}f_6(n) = \log{n-1}\end{math}\\
The above function is clearly smaller than the function below it once a simplification is appliwed to the below function. n dominates any logarithm.
\item \begin{math}f_5(n) = \log{n^n} = n \log{n}\end{math}\\
The degree of the polynomial above is smaller than that below.
\item \begin{math}f_3(n) = 4^{\log{n}} = n^2 \end{math}\\
The degree of the polynomial above is smaller than that below.
\item \begin{math}f_2(n) = (\frac{n}{\log{n}})^3=n^3(\frac{1}{\log{n}})^3\end{math}\\
The degree of the polynomial above is same as that below but the function above is has a division of logarithm which slows it down.
\item \begin{math}f_0(n) = n^3 + n^2 + n + 1\end{math}\\
No simplication of the function below that I know of can be applied but by looking at Knuth's pecking order I can see that any function of the form \begin{math}c^n\end{math} will be greater than \begin{math}n^c\end{math}.
\item \begin{math}f_8(n) = 100^{\sqrt{n}}\end{math}\\
By consulting concrete mathematics, when you have two different exponetials you need to compare the powers of the function inside the power. Since \begin{math}\sqrt{n} < n\end{math} the function above grows slower than that below.
\item \begin{math}f_4(n) = 9^{\frac{n}{2}} = 3^{n}\end{math}\\
Since \begin{math}n < 2n\end{math} the function above grows slower than that below.
\item \begin{math}f_1(n) = 3^{2n}\end{math}\\
Since all other cases have been exhausted this function above should grow the fastest.\\\\
\end{enumerate}
With use of mathematica and the definition Knuth provided (\begin{math}f(n) < g(n) \Longleftrightarrow \lim_{n\to\infty} \frac{f(n)}{g(n)} = 0\end{math}), I obtained the following results, where the row corresponds the \begin{math}f_i(n)\end{math} and  the column corresponds to \begin{math}g_j(n)\end{math}:\\
\[
    \begin{bmatrix}
        1 & 0 & \infty & \infty & 0 & \infty & \infty & \infty & 0\\
        \infty & 1 & \infty & \infty & \infty & \infty & \infty & \infty & \infty\\
        0 & 0 & 1 & \infty & 0 & \infty & \infty & \infty & 0\\
        0 & 0 & 0 & 1 & 0 & \infty & \infty & \infty & 0\\
        \infty & 0 & \infty & \infty & 1 & \infty & \infty & \infty & \infty\\
        0 & 0 & 0 & 0 & 0 & 1 & \infty & \infty & 0\\
        0 & 0 & 0 & 0 & 0 & 0 & 1 & \infty & 0\\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
        \infty & 0 & \infty & \infty & 0 & \infty & \infty & \infty & 1
    \end{bmatrix}
\]
The rows with the most zeros or likewise the column with the most infinities in them should match to the ordering of what I obtained, which we see that it does. In the workplace and in the future I will make heavy use of technology to reduce errors and introduce beck practices, so I felt like it was important to show this step.
\end{document}